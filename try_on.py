# -*- coding: utf-8 -*-
"""Try_on.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e_d203s7Nss8AZnIJ0eAwJmflVK6FBVu
"""

from google.colab import drive
drive.mount('/content/drive')



# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Dataset/Test_Data

!ls

!pip install opencv-python-headless

from PIL import Image

image_path = '/content/drive/MyDrive/Dataset/Test_Data/test_img/000010_0.jpg'
image = Image.open(image_path)

image.show()

cd test_color

!ls



"""NOW SELECT ANY OF THE IMAGE CLOTH FOR FITTING"""

from IPython.display import Image
Image("018690_1.jpg")

from IPython.display import Image
Image("013772_1.jpg")

from IPython.display import Image
Image("004710_1.jpg")

import os
os.getcwd()



"""NOW CHANNGING THE PATH FROM THE DATASET TO THE GOOGLE DRIVE WORKING DIRECTORY"""

cd ..

cd /content/drive/MyDrive/Working/Notebook.ipynb

ls



"""LINKING TO THE GITHUB REPO"""

import os
os.getcwd()

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/anoobapk/mca-project.git
# %cd mca-project

ls



"""COPYING DATASET(LOADING)"""

!mkdir Dataset

!rm -r Dataset

!mkdir Dataset

ls





"""COPYING DATASET (COLOR, EDGE, MASK, COLORMASK)"""

# copy  dataset
!cp -r /content/drive/MyDrive/Dataset/Test_Data/test_color Dataset
!cp -r /content/drive/MyDrive/Dataset/Test_Data/test_edge Dataset
!cp -r /content/drive/MyDrive/Dataset/Test_Data/test_mask Dataset
!cp -r /content/drive/MyDrive/Dataset/Test_Data/test_colormask Dataset

# copy TestData
!cp -r /content/drive/MyDrive/Dataset/Test_Data/test_img Dataset
!cp -r /content/drive/MyDrive/Dataset/Test_Data/test_pose Dataset
!cp -r /content/drive/MyDrive/Dataset/Test_Data/test_label Dataset

ls

import os
print('test image    :', len(os.listdir('./Dataset/test_img')))   # test image (person with clothes)
print('test pose     :', len(os.listdir('./Dataset/test_pose')))  # pose keypoints per test image
print('test label    :', len(os.listdir('./Dataset/test_label'))) # label (dark frame) of test image (for pose-map)
print('test color    :', len(os.listdir('./Dataset/test_color')))     # color clothes  
print('test edge     :', len(os.listdir('./Dataset/test_edge')))      # edge of clothes
print('test mask     :', len(os.listdir('./Dataset/test_mask')))      # test mask        
print('test colormask:', len(os.listdir('./Dataset/test_colormask'))) # test colormask



"""Read pose"""

# read pose
import numpy as np
import json

pose_name = "/content/drive/MyDrive/Dataset/Test_Data/test_pose/000001_0_keypoints.json"
with open(pose_name, 'r') as f:
     pose_label = json.load(f)
     pose_data = pose_label['people'][0]['pose_keypoints']
     pose_data = np.array(pose_data)
     pose_data = pose_data.reshape((-1,3))
print(pose_data)
print(len(pose_data))

pip install tensorboardX

"""SHOWING KEY POINGS ON TESTING IMAGE"""

import matplotlib.pyplot as plt
img = plt.imread('/content/drive/MyDrive/Dataset/Test_Data/test_img/000020_0.jpg')
plt.imshow(img)
i=0
for x,y,z in pose_data: 
    plt.plot(x, y, 'w.') # 'w.': color='white', marker='.'
    plt.text(x, y, str(i), color='r', fontsize=10)
    i+=1
plt.show()
print(i)



"""Semantic Segmentation: PSPNet-MobileNet-v2"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/rkuo2000/semantic-segmentation-pytorch # predict.py add output a _gray.png
# %cd semantic-segmentation-pytorch



from torchvision import transforms

from segmentation.data_loader.segmentation_dataset import SegmentationDataset
from segmentation.data_loader.transform import Rescale, ToTensor

from segmentation.predict import *
from segmentation.models import all_models

from posenet.utils import *

model_name = "pspnet_mobilenet_v2"
device = 'cuda'
batch_size = 4
n_classes = 34 
check_point_stride = 1 # store checkpoints every 1 epoch   
image_axis_minimum_size = 200

num_epochs = 0    # 1 for 1st training
                  # n for retraining
                  # 0 for detect-only
pretrained = False# True  for num_epochs=1 without logger.load_model below
                  # False for num_epochs=n with    logger.load_model below
                  # False for detect-only  with    logger.load_model below
fixed_feature = False

logger = Logger(model_name=model_name, data_name='example')

import torch
import torchvision

print("torch version:", torch.__version__)
print("torchvision version:", torchvision.__version__)

pip install transformers



from torchvision.models import mobilenet_v2
model = mobilenet_v2(pretrained=True)
inverted_residual_block = model.features[1]

"""Pose Detection: PoseNet"""

!pip install tfjs-graph-converter

file ="/content/drive/MyDrive/Dataset/Test_Data/test_pose/000001_0_keypoints.json"

pip install posenet

"""Detect label (semantic segmentation)"""

test_img_file   = '../Dataset/test_img/test_img/000001_0.jpg'
test_label_file = '../Dataset//test_label/000001_0.png'

from IPython.display import Image

import torch
from posenet.constants import *
from posenet.decode_multi import decode_multiple_poses
from posenet.models.model_factory import load_model
from posenet.utils import *

net = load_model(101)
net = net.cuda()
output_stride = net.output_stride
scale_factor = 1.0

input_image, draw_image, output_scale = posenet.read_imgfile(file, scale_factor=scale_factor, output_stride=output_stride)







"""Detect Pose keypoints"""

